# -*- coding: utf-8 -*-
"""EmpathyBot_Sprint.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ArBOUGjDcbHNbaCqZKNMt3fEEwdsoWLZ
"""

# Commented out IPython magic to ensure Python compatibility.
# Core libs
# %pip -q install -U pip
# %pip -q install transformers>=4.44.0 torch sentence-transformers>=2.7.0 datasets>=2.20.0
# %pip -q install faiss-cpu>=1.7.4 nltk python-dotenv

import nltk, os, json, random, math, itertools, numpy as np
from pathlib import Path
nltk.download('punkt', quiet=True)

# Create a working folder
WORKDIR = Path("/content/EmpathyBot")
DATA_DIR = WORKDIR /"data"; DATA_DIR.mkdir(exist_ok=True)
os.makedirs(WORKDIR, exist_ok=True)
print("Ready at", WORKDIR)

# Commented out IPython magic to ensure Python compatibility.
# === tweet_eval/emotion — ONE merged, cleaned file (no split info kept) ===
# %pip -q install -U datasets pandas pyarrow

import re, unicodedata
from pathlib import Path
import pandas as pd
from datasets import load_dataset

# Paths
WORKDIR = Path("/content/EmpathyBot"); WORKDIR.mkdir(parents=True, exist_ok=True)
DATA_DIR = WORKDIR / "data"; DATA_DIR.mkdir(parents=True, exist_ok=True)

# Emoji/cleanup helpers (same style as your RAG corpus cleaning)
EMOJI_RE = re.compile(
    "["
    "\U0001F600-\U0001F64F"  # emoticons
    "\U0001F300-\U0001F5FF"  # symbols & pictographs
    "\U0001F680-\U0001F6FF"  # transport & map
    "\U0001F1E0-\U0001F1FF"  # flags
    "\U00002700-\U000027BF"  # dingbats
    "\U000024C2-\U0001F251"  # enclosed
    "\U00010000-\U0010FFFF"  # supplementary planes
    "]+"
)
URL_RE  = re.compile(r"http[s]?://\S+|www\.\S+", re.IGNORECASE)
CTRL_RE = re.compile(r"[\u0000-\u001F\u007F]")

def clean_text(s: str) -> str:
    if not isinstance(s, str): return ""
    s = unicodedata.normalize("NFKC", s)
    s = s.lower().strip()
    s = URL_RE.sub("", s)
    s = EMOJI_RE.sub("", s)
    s = CTRL_RE.sub(" ", s)
    s = re.sub(r"[^a-z0-9\s'.,!?-]+", " ", s)  # keep simple punctuation
    s = re.sub(r"\s+", " ", s).strip()
    return s

# Load dataset
ds = load_dataset("tweet_eval", "emotion")
labels = ds["train"].features["label"].names
id2name = {i: n for i, n in enumerate(labels)}

# Map 6-class -> 4-class (optional but handy)
MAP6to4 = {
    "joy": "happiness", "love": "happiness",
    "sadness": "sadness",
    "anger": "anger",
    "fear": "neutral", "surprise": "neutral",
}

# Convert all splits → one cleaned dataframe (no split column kept)
def as_clean_df(split_name: str) -> pd.DataFrame:
    d = ds[split_name].to_pandas()
    d["text_raw"] = d["text"]
    d["text"] = d["text"].map(clean_text)
    d["label_name"] = d["label"].map(id2name)
    d["label_4"] = d["label_name"].map(MAP6to4).fillna("neutral")
    d = d[(d["text"].str.len() >= 3) & (d["text"].str.len() <= 300)]
    d = d.drop_duplicates(subset=["text"]).reset_index(drop=True)
    return d[["text_raw","text","label","label_name","label_4"]]

df_clean = pd.concat(
    [as_clean_df(s) for s in ("train","validation","test")],
    ignore_index=True
)

# Global dedupe across all data
df_clean = df_clean.drop_duplicates(subset=["text"]).reset_index(drop=True)

# Save a single merged artifact
out_parquet = DATA_DIR / "tweet_eval_emotion_merged_clean.parquet"
out_csv     = DATA_DIR / "tweet_eval_emotion_merged_clean.csv"
df_clean.to_parquet(out_parquet, index=False)
df_clean.to_csv(out_csv, index=False)

print("Rows:", len(df_clean))
print("6-class dist:", df_clean["label_name"].value_counts().to_dict())
print("4-class dist:", df_clean["label_4"].value_counts().to_dict())
print("Saved:", out_parquet, "and", out_csv)

df_clean.head(5)

# Commented out IPython magic to ensure Python compatibility.
# A1) Install + import
# %pip -q install -U datasets transformers

from datasets import load_dataset
import itertools, random, json, re, collections

# A2) Load the Parquet version
ed = load_dataset("Estwld/empathetic_dialogues_llm")  # no script loader needed
print(ed)
print(ed["train"].column_names[:5])
print(ed["train"][0])  # inspect fields (emotion, conversations, etc.)

import json, math

IN  = WORKDIR / "corpus.json"            # <-- put your file here
OUT = WORKDIR / "corpus_clean.json"
REVIEW = WORKDIR / "corpus_flagged.json"

VALID_EMOS = {"happiness","sadness","anger","neutral"}
MIN_LEN, MAX_LEN = 8, 180
ENABLE_NEAR_DUP = True
NEAR_DUP_JACCARD = 0.85   # 0..1 (higher = stricter)
TARGET_PER = None         # e.g., 250 to rebalance, or None to keep counts

CRISIS_PATTERNS = [
    r"\bsuicid(e|al)\b", r"\bkill myself\b", r"\bend my life\b",
    r"\bself[- ]?harm\b", r"\boverdose\b", r"\bcutting\b"
]
PROFANITY = [r"\b(fuck|shit|bitch|asshole|bastard)\b"]

def is_flagged(s: str) -> bool:
    low = s.lower()
    for pat in itertools.chain(CRISIS_PATTERNS, PROFANITY):
        if re.search(pat, low):
            return True
    return False

def jaccard(a: str, b: str) -> float:
    A, B = set(a.split()), set(b.split())
    if not A and not B: return 1.0
    return len(A & B) / max(1, len(A | B))

# 1) Load & validate
raw = json.loads(IN.read_text(encoding="utf-8"))
rows, flagged = [], []

for r in raw:
    emo = str(r.get("emotion","")).strip().lower()
    txt = str(r.get("template","")).strip()
    if emo not in VALID_EMOS or not txt:
        continue
    txt = clean_text(txt)
    if not (MIN_LEN <= len(txt) <= MAX_LEN):
        continue
    item = {"emotion": emo, "template": txt}
    if is_flagged(txt):
        flagged.append(item)
        continue
    rows.append(item)

# 2) Exact dedupe (per emotion+template)
seen, deduped = set(), []
for r in rows:
    key = (r["emotion"], r["template"])
    if key in seen:
        continue
    seen.add(key)
    deduped.append(r)
rows = deduped

# 3) Near-dup removal (same emotion bucket)
if ENABLE_NEAR_DUP:
    pruned = []
    kept_by_emo = {e: [] for e in VALID_EMOS}
    for r in rows:
        emo, txt = r["emotion"], r["template"]
        if any(jaccard(txt, t) >= NEAR_DUP_JACCARD for t in kept_by_emo[emo]):
            continue
        kept_by_emo[emo].append(txt)
        pruned.append(r)
    rows = pruned

# 4) Optional rebalance
if TARGET_PER:
    buckets = {e: [] for e in VALID_EMOS}
    for r in rows: buckets[r["emotion"]].append(r)
    for e in buckets:
        random.shuffle(buckets[e]); buckets[e] = buckets[e][:TARGET_PER]
    rows = [x for e in VALID_EMOS for x in buckets[e]]

# 5) Save & small report
OUT.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding="utf-8")
REVIEW.write_text(json.dumps(flagged, ensure_ascii=False, indent=2), encoding="utf-8")
from collections import Counter
print("Report:",
      {"input_total": len(raw),
       "kept_total": len(rows),
       "flagged_total": len(flagged),
       "kept_by_emotion": dict(Counter([r["emotion"] for r in rows]))})
print("Wrote:", OUT, "and flagged:", REVIEW)

# A few tweet_eval rows after cleaning
import pandas as pd
print(pd.read_parquet(DATA_DIR/"tweet_eval_train_clean.parquet").head(3))

# A few corpus rows after cleaning
with open(OUT, "r", encoding="utf-8") as f:
    cleaned = json.load(f)
print(cleaned[:5])

def collect_utterances(ds, max_per_split=10000, min_len=3, max_len=300):
    def get_text(ex):
        for k in ("utterance", "text", "context"):
            v = ex.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
        return ""
    pool = []
    for split in ["train","validation","test"]:
        if split not in ds:
            continue
        for ex in itertools.islice(ds[split], max_per_split):
            txt = get_text(ex)
            if min_len <= len(txt) <= max_len:
                pool.append(txt)
    seen, uniq = set(), []
    for t in pool:
        if t not in seen:
            uniq.append(t); seen.add(t)
    random.shuffle(uniq)
    return uniq



candidates = collect_utterances(ds, max_per_split=10000)
len(candidates), candidates[:5]

DETECTOR_MODEL = "bhadresh-savani/distilbert-base-uncased-emotion"

import torch, re, unicodedata
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline

# Basic cleaner (same style as earlier)
EMOJI_RE = re.compile("[" "\U0001F600-\U0001F64F" "\U0001F300-\U0001F5FF" "\U0001F680-\U0001F6FF"
                      "\U0001F1E0-\U0001F1FF" "\U00002700-\U000027BF" "\U000024C2-\U0001F251"
                      "\U00010000-\U0010FFFF" "]+")
URL_RE  = re.compile(r"http[s]?://\S+|www\.\S+", re.IGNORECASE)
CTRL_RE = re.compile(r"[\u0000-\u001F\u007F]")

def clean_text(s: str) -> str:
    if not isinstance(s, str): return ""
    s = unicodedata.normalize("NFKC", s)
    s = s.lower().strip()
    s = URL_RE.sub("", s)
    s = EMOJI_RE.sub("", s)
    s = CTRL_RE.sub(" ", s)
    s = re.sub(r"[^a-z0-9\s'.,!?-]+", " ", s)  # keep simple punctuation
    s = re.sub(r"\s+", " ", s).strip()
    return s


tok = AutoTokenizer.from_pretrained(DETECTOR_MODEL)
mdl_det = AutoModelForSequenceClassification.from_pretrained(DETECTOR_MODEL)
emo_pipe = TextClassificationPipeline(
    model=mdl_det,
    tokenizer=tok,
    return_all_scores=True,
    device=0 if torch.cuda.is_available() else -1,
)

# 6→4 bucket map (if you use the 6-emotion model)
MAP6to4 = {
    "joy": "happiness", "love": "happiness",
    "sadness": "sadness",
    "anger": "anger",
    "fear": "neutral", "surprise": "neutral",
}

def detect_emotion_label_and_conf(text: str):
    """Use emo_pipe (classification) — NOT gen_pipe."""
    t = clean_text(text)
    out = emo_pipe(t)


    scores = out[0] if (out and isinstance(out[0], dict) is False) else out
    # ensure we now have a list[dict]
    if scores and isinstance(scores[0], dict):
        by = {d["label"].lower(): float(d["score"]) for d in scores}
    else:
        raise RuntimeError(f"Unexpected detector output shape: {type(out)} => {out}")

    label = max(by, key=by.get)
    return label, by[label]

def to_4_bucket_with_threshold(lbl: str, conf: float, thr: float = 0.45) -> str:
    return "neutral" if conf < thr else MAP6to4.get(lbl.lower(), "neutral")

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain langchain-community sentence-transformers faiss-cpu

from langchain_core.documents import Document
import json
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Load corpus
CORPUS_PATH = WORKDIR / "corpus_clean.json"

with open(CORPUS_PATH, "r", encoding="utf-8") as f:
    corpus = json.load(f)
docs = [Document(page_content=clean_text(r["template"]),
                 metadata={"emotion": r["emotion"].lower().strip()}) for r in corpus]

# Embeddings (MiniLM) and FAISS vector store
emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vstore = FAISS.from_documents(docs, embedding=emb)
retriever = vstore.as_retriever(search_kwargs={"k": 8})  # retrieve wider; we'll prune to 3

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline as hf_pipeline
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
import torch

GEN_MODEL = "google/flan-t5-base"  # use "google/flan-t5-small" if VRAM is tight

# 1) Load with GPU/half-precision if available (saves VRAM, faster)
tok_gen = AutoTokenizer.from_pretrained(GEN_MODEL)
model_kwargs = {}
if torch.cuda.is_available():
    model_kwargs = {"torch_dtype": torch.float16, "device_map": "auto"}
mdl_gen = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL, **model_kwargs)




# 2) Build generation pipeline with anti-repetition + sane max tokens
gen_pipe = hf_pipeline(
    task="text2text-generation",
    model=mdl_gen,
    tokenizer=tok_gen,
    max_new_tokens=80,
    temperature=0.7,
    top_p=0.9,
    num_beams=4,
    no_repeat_ngram_size=3,
    repetition_penalty=1.2,
    early_stopping=True,
    # return_full_text=False  # uncomment if you only want the new text
)

# 3) Wrap in LangChain LLM
llm = HuggingFacePipeline(pipeline=gen_pipe)

# Add a light, transparent heuristic: if strong sadness cues, prefer sadness.
SAD_HINTS = {"down", "sad", "upset", "depressed", "lonely", "nothing works", "blue", "cry", "exhausted"}
ANGER_HINTS = {"furious", "angry", "unfair", "mad", "irritated", "rage"}

def heuristic_emotion_override(text: str, raw_label: str) -> str:
    t = clean_text(text)
    toks = set(t.split())
    if any(h in t for h in SAD_HINTS) and raw_label in {"anger","fear","surprise"}:
        return "sadness"
    if any(h in toks for h in ANGER_HINTS) and raw_label in {"sadness","fear","surprise"}:
        return "anger"
    return raw_label

from langchain.prompts import PromptTemplate, FewShotPromptTemplate

examples = [
    {"user":"i failed my exam and feel awful","emotion":"sadness",
     "templates":"i’m really sorry you’re feeling this way. i’m here to listen—what happened?",
     "reply":"I’m really sorry this feels so heavy. I’m here to listen—what happened?"},

    {"user":"i got the job!!","emotion":"happiness",
     "templates":"that’s fantastic—congrats! what are you most proud of?",
     "reply":"That’s fantastic—congrats! What are you most proud of?"},

    {"user":"they treated me so unfairly","emotion":"anger",
     "templates":"your frustration makes sense. what triggered it most?",
     "reply":"Your frustration makes sense. What triggered it most?"}
]

example_prompt = PromptTemplate.from_template(
    "User: {user}\nEmotion: {emotion}\nTemplates: {templates}\nReply: {reply}\n"
)

fewshot = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix=(
        "You are an empathetic assistant.\n"
        "Reply in 1–2 sentences. Acknowledge the feeling once, ask one gentle open question.\n"
        "Paraphrase; do NOT quote the user’s words. Do NOT invent facts about the user.\n"
        "Do NOT mention 'templates', 'emotion', or any guidelines. Output ONLY the reply text.\n\n"
    ),
    suffix=("User: {user}\nEmotion: {emotion}\nTemplates: {templates}\nReply:"),
    input_variables=["user","emotion","templates"],
)
DISCLAIMER='I’m not a therapist; for serious concerns or emergencies, please seek professional help immediately.'


# Quick keyword helper
STOP = {"the","a","an","and","or","but","to","for","of","in","on","at","it","is","are","i","you","me","my","your","that","this","was","were"}
def keywords(s: str):
    return {w for w in re.findall(r"[a-z']+", clean_text(s)) if len(w) > 2 and w not in STOP}

# Emotion-specific safe fallback lines (used if retrieval is poor)
FALLBACKS = {
    "happiness": [
        "That’s wonderful—what’s making you smile most right now?",
        "Congrats! What are you most proud of?"
    ],
    "sadness": [
        "I’m really sorry you’re feeling this way. Want to share what’s weighing on you?",
        "That sounds heavy. I’m here to listen—what happened?"
    ],
    "anger": [
        "I hear your frustration. What felt most unfair about it?",
        "That sounds upsetting. What triggered it the most?"
    ],
    "neutral": [
        "Thanks for sharing. Want to tell me more so I can understand better?",
        "I’m listening—what matters most about this for you?"
    ],
}

def is_on_topic(template: str, user_text: str, min_overlap: int = 1, off_topic_terms=None):
    utoks = keywords(user_text)
    ttoks = keywords(template)
    if not ttoks: return False
    # overlap
    if len(utoks & ttoks) < min_overlap and len(utoks) >= 3:
        return False
    # avoid obvious mismatches (exam/degree/lunch unless user mentions them)
    off_topic_terms = off_topic_terms or {"exam","degree","lunch","todayl"}
    if (ttoks & off_topic_terms) and not (utoks & off_topic_terms):
        return False
    return True

def retrieve_top3(user_text: str, target_emotion_4: str, k: int = 3):
    # Strong emotion filter first; widen for recall
    hits = vstore.similarity_search(user_text, k=k*8, filter={"emotion": target_emotion_4})
    if not hits:
        hits = retriever.get_relevant_documents(user_text)

    out, seen = [], set()
    for d in hits:
        t = d.page_content.strip()
        if t in seen:
            continue
        if not is_on_topic(t, user_text):
            continue
        if len(t.split()) < 5:       # very short fragments aren’t helpful
            continue
        seen.add(t); out.append(t)
        if len(out) >= k:
            break

    if len(out) < k:
        # top up with safe fallbacks for the emotion bucket
        for fb in FALLBACKS.get(target_emotion_4, FALLBACKS["neutral"]):
            if fb not in seen:
                out.append(fb)
                seen.add(fb)
                if len(out) >= k: break
    return out[:k]

META_PATTERNS = [
    r"\btemplates?\b", r"\bemotion:\b", r"\buser:\b",
    r"\bguideline(s)?\b", r"\bdo not\b", r"\bparaphrase\b"
]

def too_similar(a: str, b: str) -> bool:
    ak, bk = keywords(a), keywords(b)
    if not ak or not bk: return False
    overlap = len(ak & bk) / max(1, len(ak | bk))
    return overlap > 0.7  # if >70% keyword overlap, it's basically a restatement

def postprocess(text: str, user_text: str) -> str:
    text = text.strip()
    # strip meta lines
    lines = [ln for ln in text.splitlines() if ln.strip()]
    lines = [ln for ln in lines if not any(re.search(p, ln.lower()) for p in META_PATTERNS)]
    text = " ".join(lines).strip()

    # split sentences, dedupe, cap 2
    sents = re.split(r'(?<=[.!?])\s+', text)
    uniq, seen = [], set()
    for s in sents:
        s2 = s.strip()
        if not s2: continue
        k = s2.lower()
        if k in seen: continue
        seen.add(k); uniq.append(s2)
        if len(uniq) >= 2: break
    out = " ".join(uniq)

    # avoid heavy parroting of the user
    if too_similar(out, user_text):
        out = re.sub(r"\bi\b'm\b.*", "", out, flags=re.I).strip()

    # de-stutter “I’m sorry”
    out = re.sub(r"(i(?:'m| am) sorry[, ]*){2,}", "I’m sorry, ", out, flags=re.I)

    return out.strip()

OPENERS = {
    "happiness": "That’s great to hear.",
    "sadness": "I’m really sorry this feels so heavy.",
    "anger": "I hear your frustration.",
    "neutral": "Thanks for sharing."
}
FOLLOWUPS = {
    "happiness": "What are you most proud of?",
    "sadness": "Want to share what’s weighing on you?",
    "anger": "What felt most unfair about it?",
    "neutral": "Want to tell me more so I can understand better?"
}

def compose_from_templates(bucket: str, templates: list[str]) -> str:
    opener = OPENERS.get(bucket, OPENERS["neutral"])
    ask = FOLLOWUPS.get(bucket, FOLLOWUPS["neutral"])
    main = templates[0].strip().rstrip(".")
    # small paraphrase-ish join
    return f"{opener} {main.capitalize()}. {ask}"

def respond(
    user_message: str,
    *,
    force_emotion: str | None = None,
    k: int = 3
):
    # 1) detect with confidence & heuristic override
    if force_emotion:
        raw_label, conf = force_emotion, 1.0
    else:
        raw_label, conf = detect_emotion_label_and_conf(user_message)
        raw_label = heuristic_emotion_override(user_message, raw_label)
    bucket = to_4_bucket_with_threshold(raw_label, conf, thr=0.50)

    # 2) retrieve k templates (on-topic + emotion)
    cands = retrieve_top3(user_message, bucket, k=k)
    if not cands:
        cands = FALLBACKS[bucket][:k]

    # 3) generate with few-shot (use .invoke)
    prompt = fewshot.format(user=user_message, emotion=bucket, templates=" | ".join(cands))
    raw = llm.invoke(prompt).strip()

    # 4) post-process; if weak/empty/echo, synthesize from templates
    final = postprocess(raw, user_message)
    if (len(final.split()) < 4
        or "you are an empathetic assistant" in final.lower()
        or too_similar(final, user_message)):
        final = compose_from_templates(bucket, cands)

    return {
        "detected_emotion": raw_label,
        "confidence": round(conf, 3),
        "bucket": bucket,
        "templates": cands,
        "reply": f"{final}\n\n{DISCLAIMER}",
    }

tests = [
    "I’m really down today. Nothing seems to work.",
    "I aced my exam and I’m so happy!",
    "I’m furious about how unfairly I was treated.",
    "Honestly I’m just okay, not much to say."
    ,"I am so happy , today was my wedding"
]
for t in tests:
    res = respond(t)
    print(f"\nUSER: {t}\nDetected: {res['detected_emotion']} → bucket={res['bucket']}")
    print("Templates:", res["templates"])
    print("REPLY:", res["reply"])